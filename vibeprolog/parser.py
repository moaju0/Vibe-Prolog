"""Prolog parser using Lark."""

import re
from collections import defaultdict
from dataclasses import dataclass
from typing import Any, Iterable

from lark import Lark, Transformer, v_args
from lark.exceptions import LarkError, UnexpectedCharacters, UnexpectedToken

from vibeprolog.exceptions import PrologError, PrologThrow
from vibeprolog.operator_defaults import DEFAULT_OPERATORS
from vibeprolog.terms import Atom, Variable, Number, Compound

# ISO Prolog graphic characters that can form operators
# When /* is immediately preceded by one of these, it's part of an operator token, not a comment
GRAPHIC_CHARS = set('#$&*+-./:<=>?@^~\\')


def _is_graphic_char(ch: str) -> bool:
    """Return True if ch is an ISO Prolog graphic character."""
    return ch in GRAPHIC_CHARS


def _should_start_block_comment(text: str, index: int, prev_char: str | None) -> bool:
    """Return True if /* at ``index`` starts a block comment based on context."""

    # Graphic operators like //* rely on the preceding slash to keep the token intact.
    if prev_char == '/':
        return False
    # Graphic operators like /*/ must remain intact even when not preceded by '/'.
    if text.startswith('/*/', index):
        return False
    return True


@dataclass(frozen=True)
class List:
    """A list."""

    elements: tuple[Any, ...]
    tail: Any = None  # For [H|T] syntax

    def __repr__(self):
        if not self.elements and self.tail is None:
            return "[]"
        if self.tail is not None:
            elements_str = ", ".join(str(e) for e in self.elements)
            return f"[{elements_str}|{self.tail}]"
        elements_str = ", ".join(str(e) for e in self.elements)
        return f"[{elements_str}]"


@dataclass(frozen=True)
class Cut:
    """The cut operator (!)."""

    def __repr__(self):
        return "!"


@dataclass(frozen=True)
class ParenthesizedComma(Compound):
    """Marker type for comma terms wrapped in parentheses to prevent flattening."""


@dataclass
class Clause:
    """A Prolog clause (fact or rule)."""

    head: Compound
    body: list[Compound] | None = None  # None for facts
    doc: str | None = None  # PlDoc documentation
    meta: Any = None  # Lark meta information
    dcg: bool = False  # True for DCG rules

    def is_fact(self):
        return self.body is None

    def is_rule(self):
        return self.body is not None


@dataclass
class Directive:
    """A Prolog directive (e.g., :- initialization(goal).)."""

    goal: Any  # The directive term, e.g., initialization(goal)
    doc: str | None = None  # PlDoc documentation
    meta: Any = None  # Lark meta information


@dataclass
class PredicateIndicator:
    """A predicate indicator Name/Arity."""

    name: Any
    arity: Any


@dataclass
class PredicatePropertyDirective:
    """Directive declaring predicate properties such as dynamic/1."""

    property: str
    indicators: tuple[PredicateIndicator, ...]


# Lark grammar for Prolog
PROLOG_GRAMMAR = r"""
    start: (clause | directive)+

    clause: rule | dcg_rule | fact
    fact: term "."
    rule: term ":-" goals "."
    dcg_rule: term DCG_ARROW goals "."
    atom_or_compound: atom
        | atom "(" args ")"

    directive: ":-" (op_directive | prefix_directive | property_directive | term) "."

    prefix_directive: "dynamic" predicate_indicators -> dynamic_directive
        | "multifile" predicate_indicators -> multifile_directive
        | "discontiguous" predicate_indicators -> discontiguous_directive

    property_directive: "dynamic" "(" predicate_indicators ")"    -> dynamic_directive
        | "multifile" "(" predicate_indicators ")"   -> multifile_directive
        | "discontiguous" "(" predicate_indicators ")" -> discontiguous_directive

    op_directive: "op" "(" number "," ATOM "," op_arg_list ")" -> op_directive

    op_arg_list: op_symbol_or_atom
        | "[" op_symbol_or_atom ("," op_symbol_or_atom)* "]" -> op_arg_list_items

    // operator symbol or atom for op/3 - avoid parsing as expressions
    op_symbol_or_atom: OP_SYMBOL_DIRECTIVE | OP_SYMBOL | ATOM | SPECIAL_ATOM | OPERATOR_ATOM

    // Operator symbols - must match complete sequences before breaking into component operators
    // Priority set to ensure special atoms like -$ are recognized, but still below SPECIAL_ATOM_OPS
    // NOTE: .. (range operator) must have HIGH priority to prevent being split into . . by lexer
    RANGE_OP.30: /\.\./
    OP_SYMBOL_DIRECTIVE.25: /[+\-*\/<>=\\@#$&!~:?^.]+/
    OP_SYMBOL: /[+\-*\/<>=\\@#$&!~:?^.]+/

    predicate_indicators: term ("," term)*

    goals: term ("," term)*

__OPERATOR_GRAMMAR__

    primary: SPECIAL_ATOM_OPS -> special_atom_token
        | operator_atom
        | operator_compound
        | compound
        | curly_braces
        | string
        | cut
        | char_code
        | number
        | "-" number -> negative_number
        | atom
        | variable
        | list
        | "(" operator_as_atom ")"          -> parenthesized_operator_atom
        | "(" term ")"                     -> parenthesized_term

    compound: atom "(" args ")"
    // Allow operator symbols as functors: ;(a,b), |(a,b), ,(a,b), ->(a,b), etc.
    operator_compound: operator_functor "(" args ")" -> operator_compound
    // Operators that can be used as functors when followed by (
    operator_functor: INFIX_OP_FUNCTOR | CONTROL_OP_FUNCTOR | COMPARISON_OP_FUNCTOR | ARITH_OP_FUNCTOR | OP_SYMBOL
    // Infix operators like ;, |, ,, ->, etc.
    INFIX_OP_FUNCTOR.30: /;/ | /\|/ | /,/ | /->/ | /:/ | /=/ 
    // Control operators
    CONTROL_OP_FUNCTOR.30: /\\+/
    // Comparison operators - but not < and > alone as they're in OP_SYMBOL
    COMPARISON_OP_FUNCTOR.30: /=:=/ | /=\\=/ | /=</ | />=/ | /=@=/ | /\\=@=/ | /==/ | /\\==/
    // Arithmetic operators that can also be prefix/postfix - must be followed by ( for functor use
    ARITH_OP_FUNCTOR.30: /\+/ | /-/ | /\*/ | /\//
    // Parenthesized operator as atom: (;), (|), (,), (->), etc.
    operator_as_atom: INFIX_OP_FUNCTOR | CONTROL_OP_FUNCTOR | COMPARISON_OP_FUNCTOR | ARITH_OP_FUNCTOR | OP_SYMBOL
    operator_atom: OPERATOR_ATOM
    args: term ("," term)*

    curly_braces: "{" term "}"

    DCG_ARROW.35: "-->"
    OPERATOR_ATOM.35: ":-"

    list: "[" "]"                          -> empty_list
        | "[" list_items "]"               -> list_items_only
        | "[" list_items "|" term "]"      -> list_with_tail

    list_items: term ("," term)*

    string: STRING
    atom: ATOM | SPECIAL_ATOM | SPECIAL_ATOM_OPS | OP_SYMBOL
    variable: VARIABLE
    char_code: CHAR_CODE
    number: NUMBER
    cut: "!"

    // Character codes: 0'X where X is any character (must come before NUMBER)
    // Allow ISO escapes (hex, unicode, octal, and named escapes), doubled quotes,
    // or a single literal character. Optional trailing quote is accepted for
    // compatibility but limited to a single character/escape so we don't consume
    // following tokens like the clause terminator.
    CHAR_CODE.5: /0'(?:''|\\x[0-9A-Fa-f]+\\?|\\u[0-9A-Fa-f]{4}|\\[0-7]{1,3}|\\[abdefnrstvs\\\"']|[^'\\])'?/ | /[1-9]\d*'.'/

    STRING: /(?s)"(\\.|[^"])*"/
    SPECIAL_ATOM: /(?s)'(\\.|''|[^'])*'/

    // Special atom operators must have HIGHEST priority to prevent being parsed as prefix operators
    SPECIAL_ATOM_OPS.12: /-\$/ | /\$-/

    // Scientific notation, hex, octal, binary, Edinburgh <radix>'<number>
    NUMBER.4: /-?0x[0-9a-fA-F]+/i
            | /-?0o[0-7]+/i
            | /-?0b[01]+/i
            | /-?[\d_]+\.?[\d_]*[eE][+-]?[\d_]+/
            | /-?[\d_]+\.[\d_]+/
            | /-?\.[\d_]+/
            | /-?\d+['][a-zA-Z0-9_]+/
            | /-?(?=[\d_]*\d)[\d_]+/

    ATOM: /[a-z][a-zA-Z0-9_]*/ | /\{\}/ | /\$[a-zA-Z0-9_-]*/ | /[+\-*\/]/

    VARIABLE: /[A-Z_][a-zA-Z0-9_]*/

    %import common.WS
    %ignore WS
    %ignore /%.*/  // Line comments
    // Block comments are handled by Python pre-processing in _collect_pldoc_comments
    // which properly handles nesting, PlDoc, and the slash-prefix guard rules
"""


class PrologTransformer(Transformer):
    """Transform parse tree into AST."""

    def __init__(self):
        super().__init__()
        self._anon_counter = 0

    def start(self, items):
        return items

    def clause(self, items):
        return items[0]

    def atom_or_compound(self, items):
        if len(items) == 1:
            return items[0]  # Just an atom
        else:
            # atom "(" args ")"
            atom = items[0]
            args = items[1]
            return Compound(atom.name, tuple(args))

    @v_args(meta=True)
    def directive(self, meta, items):
        return Directive(goal=items[0], meta=meta)

    def predicate_indicators(self, items):
        return items

    def dynamic_directive(self, items):
        indicators = self._flatten_comma_separated(items[0])
        return PredicatePropertyDirective("dynamic", tuple(indicators))

    def multifile_directive(self, items):
        indicators = self._flatten_comma_separated(items[0])
        return PredicatePropertyDirective("multifile", tuple(indicators))

    def discontiguous_directive(self, items):
        indicators = self._flatten_comma_separated(items[0])
        return PredicatePropertyDirective("discontiguous", tuple(indicators))

    def op_directive(self, items):
        """Handle op/3 directive: op(Precedence, Type, Name).
        
        Creates a Compound term op(Prec, Type, Name) that the engine will process.
        """
        # items[0] = number (precedence)
        # items[1] = atom (type: xfx, yfx, etc.) - raw token
        # items[2] = op_arg_list (either single atom or list of atoms)
        precedence = items[0]
        op_type_token = items[1]
        op_names = items[2]
        
        # Convert operator type token to Atom
        op_type = Atom(str(op_type_token))
        
        # op_names is either a single Atom or a list of Atoms
        if isinstance(op_names, list):
            # op_arg_list_items case - convert to list representation
            return Compound("op", (precedence, op_type, List(elements=tuple(op_names))))
        else:
            # Single op_symbol_or_atom case
            return Compound("op", (precedence, op_type, op_names))

    def op_arg_list(self, items):
        """Handle operator argument list - either single or bracketed list."""
        return items[0]

    def op_arg_list_items(self, items):
        """Handle list of operators: [op1, op2, ...]"""
        return items

    def op_symbol_or_atom(self, items):
        """Operator symbol or atom - convert raw token to Atom."""
        token = items[0]
        if isinstance(token, Atom):
            return token
        else:
            # Raw token from OP_SYMBOL, ATOM, SPECIAL_ATOM, or OPERATOR_ATOM
            s = str(token)
            # Remove quotes if present (for SPECIAL_ATOM)
            if (s.startswith("'") and s.endswith("'")):
                s = s[1:-1]
                # In Prolog, single quotes are escaped by doubling them
                s = s.replace("''", "'")
            return Atom(s)

    def _flatten_comma_separated(self, items):
        """Flatten comma-separated predicates into a list.

        If items is a list with a single Compound with functor ',',
        unwrap it into a flat list of indicators.
        """
        if isinstance(items, ParenthesizedComma):
            items = Compound(items.functor, items.args)
        if isinstance(items, Compound) and items.functor == ',':
            return self._collect_comma_terms(items)
        if isinstance(items, (list, tuple)) and len(items) == 1:
            return self._flatten_comma_separated(items[0])
        if isinstance(items, (list, tuple)):
            return items
        return [items]

    def _collect_comma_terms(self, compound):
        """Recursively collect terms from a comma compound."""
        if isinstance(compound, ParenthesizedComma):
            return [Compound(compound.functor, compound.args)]
        if isinstance(compound, Compound) and compound.functor == ',':
            left = self._collect_comma_terms(compound.args[0])
            right = self._collect_comma_terms(compound.args[1])
            return left + right
        else:
            return [compound]

    @v_args(meta=True)
    def fact(self, meta, items):
        return Clause(head=items[0], body=None, meta=meta)

    @v_args(meta=True)
    def rule(self, meta, items):
        head, body = items
        return Clause(head=head, body=body, meta=meta)

    @v_args(meta=True)
    def dcg_rule(self, meta, items):
        head = items[0]
        body = items[-1]
        return Clause(head=head, body=body, dcg=True, meta=meta)

    def goals(self, items):
        # Flatten any comma compounds in the goal list
        # This handles cases where ambiguous parsing creates comma compounds
        result = []
        for item in items:
            if isinstance(item, ParenthesizedComma):
                item = Compound(item.functor, item.args)
            if isinstance(item, Compound) and item.functor == ',':
                result.extend(self._collect_comma_terms(item))
            else:
                result.append(item)
        return result

    def term(self, items):
        return items[0]

    def or_term(self, items):
        if len(items) == 1:
            return items[0]
        # Build right-associative tree for disjunction
        result = items[-1]
        for i in range(len(items) - 2, -1, -1):
            result = Compound(";", (items[i], result))
        return result

    def if_then_term(self, items):
        if len(items) == 1:
            return items[0]
        # items[0] -> items[1]
        return Compound("->", (items[0], items[1]))

    def and_term(self, items):
        if len(items) == 1:
            return items[0]
        # Build right-associative tree for conjunction
        result = items[-1]
        for i in range(len(items) - 2, -1, -1):
            result = Compound(",", (items[i], result))
        return result

    def comparison_term(self, items):
        if len(items) == 1:
            return items[0]
        left, op, right = items
        return Compound(str(op), (left, right))

    def prefix_term(self, items):
        return items[0]

    def prefix_op(self, items):
        # items can be [op, term] or just [term] depending on the rule
        if len(items) == 2:
            op, term = items
            return self._apply_prefix_operator(op, term)
        else:
            # For "-" prefix_term rule, items is just [term]
            # The "-" is implicit in the rule
            term = items[0]
            return self._apply_prefix_operator("-", term)

    def _apply_prefix_operator(self, op, term):
        op_str = str(op)
        if op_str == "-":
            if isinstance(term, Number):
                return Number(-term.value)
            if isinstance(term, Compound) and term.functor == "-" and len(term.args) == 1:
                inner = term.args[0]
                if isinstance(inner, Number):
                    return inner
        return Compound(op_str, (term,))

    def module_term(self, items):
        # Module qualification: left:right (right-associative)
        if len(items) == 1:
            return items[0]
        left = items[0]
        right = items[1]
        return Compound(":", (left, right))

    def expr(self, items):
        return items[0]

    def add_expr(self, items):
        if len(items) == 1:
            return items[0]
        # Build left-associative tree
        result = items[0]
        for i in range(1, len(items), 2):
            op = items[i]
            right = items[i + 1]
            result = Compound(str(op), (result, right))
        return result

    def infix_xfx(self, items):
        left, op, right = items
        return Compound(str(op), (left, right))

    def infix_yfx(self, items):
        left, op, right = items
        return Compound(str(op), (left, right))

    def infix_xfy(self, items):
        left, op, right = items
        return Compound(str(op), (left, right))

    def infix_yfy(self, items):
        left, op, right = items
        return Compound(str(op), (left, right))

    def prefix_fx(self, items):
        op, term = items
        return self._apply_prefix_operator(op, term)

    def prefix_fy(self, items):
        op, term = items
        return self._apply_prefix_operator(op, term)

    def postfix_xf(self, items):
        term, op = items
        return Compound(str(op), (term,))

    def postfix_yf(self, items):
        term, op = items
        return Compound(str(op), (term,))

    def mul_expr(self, items):
        if len(items) == 1:
            return items[0]
        # Build left-associative tree
        result = items[0]
        for i in range(1, len(items), 2):
            op = items[i]
            right = items[i + 1]
            result = Compound(str(op), (result, right))
        return result

    def pow_expr(self, items):
        if len(items) == 1:
            return items[0]
        # Build right-associative tree for exponentiation
        result = items[-1]
        for i in range(len(items) - 2, -1, -2):
            if i > 0:
                result = Compound(str(items[i]), (items[i - 1], result))
        if len(items) % 2 == 0:
            # If even number of items, first item hasn't been added yet
            result = Compound(str(items[1]), (items[0], result))
        return result

    def unary_expr(self, items):
        return items[0]

    def primary(self, items):
        return items[0]

    def parenthesized_term(self, items):
        term = items[0]
        if isinstance(term, Compound) and term.functor == ',':
            return ParenthesizedComma(term.functor, term.args)
        return term

    def special_atom_token(self, items):
        """Convert SPECIAL_ATOM_OPS token to an Atom."""
        token_str = str(items[0])
        return Atom(token_str)

    def parenthesized_operator_atom(self, items):
        """Handle parenthesized operator as atom: (;), (|), (,), (->), etc."""
        token_str = str(items[0])
        return Atom(token_str)

    def operator_as_atom(self, items):
        """Convert operator token to Atom."""
        token_str = str(items[0])
        return Atom(token_str)

    def operator_functor(self, items):
        """Convert operator token to Atom for use as functor."""
        token_str = str(items[0])
        return Atom(token_str)

    def operator_compound(self, items):
        """Handle operator as functor: ;(a, b), |(a, b), etc."""
        functor = items[0]
        args = items[1] if len(items) > 1 else []
        if isinstance(functor, Atom):
            return Compound(functor.name, tuple(args))
        return Compound(str(functor), tuple(args))

    def compound(self, items):
        functor = items[0]
        args = items[1] if len(items) > 1 else []
        return Compound(functor.name, tuple(args))

    def args(self, items):
        expanded: list[Any] = []
        for item in items:
            if isinstance(item, ParenthesizedComma):
                expanded.append(Compound(item.functor, item.args))
                continue

            if isinstance(item, Compound) and item.functor == ',':
                expanded.extend(self._collect_comma_terms(item))
                continue

            if isinstance(item, Compound):
                normalized_args = tuple(
                    Compound(arg.functor, arg.args)
                    if isinstance(arg, ParenthesizedComma)
                    else arg
                    for arg in item.args
                )
                item = Compound(item.functor, normalized_args)
            expanded.append(item)
        return expanded

    def empty_list(self, items):
        return List(elements=())

    def list_items_only(self, items):
        return List(elements=tuple(items[0]))

    def list_with_tail(self, items):
        elements = items[0]
        tail = items[1]
        return List(elements=tuple(elements), tail=tail)

    def list_items(self, items):
        expanded: list[Any] = []
        for item in items:
            if isinstance(item, Compound) and item.functor == ',':
                expanded.extend(self._collect_comma_terms(item))
            else:
                expanded.append(item)
        return expanded

    def string(self, items):
        # Remove quotes from the string (both double and single quotes)
        s = str(items[0])
        if s.startswith('"') and s.endswith('"'):
            s = s[1:-1]
            # Handle escape sequences in double-quoted strings
            s = self._unescape_string(s)
        elif s.startswith("'") and s.endswith("'"):
            s = s[1:-1]
            # In Prolog, single quotes are escaped by doubling them
            s = s.replace("''", "'")
            # Also handle backslash escapes
            s = self._unescape_string(s)
        return Atom(s, quoted=True)

    def _parse_escape_sequence(self, s, pos, *, allow_control_escape: bool = True):
        """Parse a single escape sequence starting at pos in string s.

        Returns (char_code, new_pos) where char_code is the Unicode code point,
        and new_pos is the position after the escape sequence.

        Raises ValueError for invalid escape sequences.
        """
        if pos >= len(s) or s[pos] != '\\':
            raise ValueError("Not an escape sequence")

        pos += 1  # Skip the backslash

        if pos >= len(s):
            raise ValueError("Incomplete escape sequence")

        ch = s[pos]
        pos += 1

        def _is_hex_digit(char: str) -> bool:
            return char.isdigit() or "a" <= char.lower() <= "f"

        # Octal escapes: up to 3 octal digits (0-7) following backslash
        if ch in "01234567":
            digits = ch
            while pos < len(s) and s[pos] in "01234567":
                digits += s[pos]
                pos += 1
            return int(digits, 8), pos

        # Hex escapes: \x followed by one or more hex digits, optionally closed
        # with a trailing backslash.
        if ch == "x":
            if pos >= len(s) or not _is_hex_digit(s[pos]):
                raise ValueError("hex_digit_missing")
            digits = ""
            while pos < len(s) and _is_hex_digit(s[pos]):
                digits += s[pos]
                pos += 1
            if len(digits) < 2:
                terminator = s[pos] if pos < len(s) else None
                if terminator is None or terminator == "\\":
                    raise ValueError("hex_digits_too_short")
                raise ValueError("invalid_hex_digit")
            if pos < len(s) and s[pos] == "\\":
                pos += 1
            return int(digits, 16), pos

        # Unicode escapes: \uXXXX (exactly 4 hex digits)
        if ch.lower() == "u":
            if pos + 4 > len(s):
                raise ValueError("Incomplete Unicode escape")
            digits = s[pos : pos + 4]
            if not all(_is_hex_digit(c) for c in digits):
                raise ValueError("Invalid Unicode escape")
            pos += 4
            return int(digits, 16), pos

        if ch == "c":
            if not allow_control_escape:
                raise ValueError("invalid_escape")
            return None, pos

        # Single character escapes
        single_char_escapes = {
            "a": 7,  # alert (bell)
            "b": 8,  # backspace
            "d": 127,  # delete
            "e": 27,  # escape
            "f": 12,  # form feed
            "n": 10,  # newline
            "r": 13,  # carriage return
            "s": 32,  # space
            "t": 9,  # tab
            "v": 11,  # vertical tab
            "'": ord("'"),  # escaped single quote
            '"': ord('"'),  # escaped double quote
            "\\": ord("\\"),  # escaped backslash
        }
        if ch in single_char_escapes:
            return single_char_escapes[ch], pos

        raise ValueError(f"Unknown escape: \\{ch}")

    def _unescape_string(self, s):
        """Unescape a string literal, handling backslash escapes."""
        res = []
        i = 0
        while i < len(s):
            if s[i] == '\\':
                if i + 1 < len(s) and s[i + 1] in "\r\n":
                    # Line continuation - skip backslash, newline, and following
                    # indentation whitespace.
                    i += 2
                    if s[i - 1] == "\r" and i < len(s) and s[i] == "\n":
                        i += 1
                    while i < len(s) and s[i] in " \t":
                        i += 1
                    continue
                code, i = self._parse_escape_sequence(s, i)
                if code is not None:  # \c produces no output
                    res.append(chr(code))
            else:
                res.append(s[i])
                i += 1
        return ''.join(res)

    def atom(self, items):
        atom_str = str(items[0])
        # Note: A single '.' is valid as an atom when used in expression context
        # (e.g., as an argument to a functor). The clause splitter already ensures
        # dots in parenthesized contexts are not treated as clause terminators.
        # Handle SPECIAL_ATOM (quoted atoms like ';', '|', etc.)
        if atom_str.startswith("'") and atom_str.endswith("'") and len(atom_str) >= 2:
            # Strip the quotes and handle escape sequences
            atom_str = atom_str[1:-1]
            # In Prolog, single quotes are escaped by doubling them
            atom_str = atom_str.replace("''", "'")
            # Handle backslash escapes
            atom_str = self._unescape_string(atom_str)
        return Atom(atom_str)

    def operator_atom(self, items):
        return Atom(str(items[0]))

    def variable(self, items):
        var_name = str(items[0])
        # Each anonymous variable _ should be unique
        if var_name == "_":
            unique_name = f"_G{self._anon_counter}"
            self._anon_counter += 1
            return Variable(unique_name)
        return Variable(var_name)

    def number(self, items):
        value = str(items[0])
        # Handle hex, octal, binary
        if value.startswith(("-0x", "0x")) or value.startswith(("-0X", "0X")):
            # Hex number
            return Number(int(value, 16))
        elif value.startswith(("-0o", "0o")) or value.startswith(("-0O", "0O")):
            # Octal number
            return Number(int(value, 8))
        elif value.startswith(("-0b", "0b")) or value.startswith(("-0B", "0B")):
            # Binary number
            return Number(int(value, 2))
        elif "'" in value:
            # Base'digits syntax
            return self._parse_base_number(value)
        # Validate underscores and strip them for numeric conversion
        self._validate_underscore_placement(value)
        clean_value = value.replace('_', '')
        if "e" in value.lower() or "." in value:
            # Scientific notation or float
            return Number(float(clean_value))
        else:
            # Regular integer
            return Number(int(clean_value))

    def negative_number(self, items):
        # The grammar rule `primary: "-" number -> negative_number` ensures
        # that the number has already been transformed into a Number object.
        num = items[-1]
        assert isinstance(num, Number), f"Expected Number, got {type(num).__name__}"
        return Number(-num.value)

    def _validate_underscore_placement(self, value: str, special_chars: str = ".eE+-") -> None:
        """Validate underscore placement in number strings."""
        if '_' not in value:
            return
        # No leading or trailing underscores
        if value.startswith('_') or value.endswith('_'):
            raise PrologThrow(PrologError.syntax_error("invalid underscore placement", "number/1"))
        # No double underscores
        if '__' in value:
            raise PrologThrow(PrologError.syntax_error("invalid underscore placement", "number/1"))
        # No underscores adjacent to special characters
        for char in special_chars:
            if f'_{char}' in value or f'{char}_' in value:
                raise PrologThrow(PrologError.syntax_error("invalid underscore placement", "number/1"))

    def _parse_base_number(self, value):
        """Parse Edinburgh <radix>'<number> syntax like 16'ff or -2'abcd.

        Edinburgh syntax: <radix>'<number> where:
        - <radix> is the base (2-36)
        - <number> is the digits in that base

        Examples: 16'ff' (hex), 2'1010' (binary), 36'ZZZ' (base-36)
        """
        # Validate underscores in Edinburgh-style base numbers
        self._validate_underscore_placement(value, special_chars="'")
        # Handle negative sign
        negative = value.startswith('-')
        if negative:
            value = value[1:]

        # Split by '
        parts = value.split("'", 1)
        base_str, digits_str = parts
        # Base must be a valid integer; token regex guarantees digits for base
        base = int(base_str)
        if not (2 <= base <= 36):
            raise ValueError(f"Base must be between 2 and 36, got {base}")

        # Remove underscores from digits
        digits_clean = digits_str.replace('_', '')

        if not digits_clean:
            raise ValueError(f"Empty digits in {value}")

        # Validate and accumulate value
        result = 0
        for digit in digits_clean.lower():
            # Convert single digit in base up to 36
            digit_val = int(digit, 36)
            if digit_val >= base:
                raise ValueError(f"Invalid digit '{digit}' for radix {base} in Edinburgh syntax: {value}")
            result = result * base + digit_val

        if negative:
            result = -result

        return Number(result)

    def char_code(self, items):
        """Handle character code notation like 0'X.

        Note: base'char'number syntax (e.g., 16'mod'2) is intentionally not implemented.
        """
        code_str = str(items[0])

        # Check for base'char format (e.g., 16'mod'2)
        if "'" in code_str and not code_str.startswith("0'"):
            # This is base'char format - extract just the character
            parts = code_str.split("'")
            if len(parts) >= 2:
                if not parts[1]:
                    # Empty character code - reject as syntax error
                    raise ValueError("unexpected_char")
                char = parts[1][0]
                return Number(ord(char))

        # Standard 0'X format
        if code_str.startswith("0'"):
            char_part = code_str[2:]

            # Strip a trailing quote used as a closing delimiter, but keep escapes that
            # intentionally include a quote character ('' or \\').
            if (
                len(char_part) > 1
                and char_part.endswith("'")
                and char_part not in {"''", "\\'"}
            ):
                char_part = char_part[:-1]

            # Reject empty character codes, including 0''
            if not char_part or char_part == "'" or char_part == "''":
                raise ValueError("unexpected_char")

            # Handle backslash escape sequences
            if char_part.startswith("\\"):
                try:
                    code, new_pos = self._parse_escape_sequence(
                        char_part, 0, allow_control_escape=False
                    )
                    if new_pos != len(char_part):
                        raise ValueError("trailing characters after escape sequence")
                    if code is None or char_part == "\\c":
                        raise ValueError("invalid_escape")
                    return Number(code)
                except ValueError as exc:
                    error_msg = str(exc)
                    if error_msg == "hex_digits_too_short":
                        raise ValueError("incomplete_reduction")
                    raise ValueError("unexpected_char")

            # Regular character
            if char_part:
                return Number(ord(char_part[0]))

        raise ValueError("unexpected_char")

    def curly_braces(self, items):
        """Handle curly braces {X} which is syntax sugar for {}(X)"""
        return Compound("{}", (items[0],))

    def cut(self, items):
        return Cut()


def tokenize_prolog_statements(prolog_code: str) -> list[str]:
    """Split Prolog code into individual clause/directive strings.

    Handles quoted strings, comments, and decimal points in numbers to avoid
    splitting valid clauses. Each returned string ends with a period.
    """
    chunks = []
    current: list[str] = []
    i = 0
    in_single_quote = False
    in_double_quote = False
    in_line_comment = False
    in_block_comment = 0
    in_char_code = False
    has_code = False
    paren_depth = 0
    bracket_depth = 0
    brace_depth = 0

    def _consume_escape_sequence(start: int) -> int:
        """Advance index past an escape sequence in quoted text."""
        next_idx = start + 1
        if next_idx >= len(prolog_code):
            return start + 1

        ch = prolog_code[next_idx]

        # Line continuation: backslash + newline + optional indentation
        if ch in "\r\n":
            idx = next_idx + 1
            if ch == "\r" and idx < len(prolog_code) and prolog_code[idx] == "\n":
                idx += 1
            while idx < len(prolog_code) and prolog_code[idx] in " \t":
                idx += 1
            current.extend(prolog_code[start:idx])
            return idx

        # Hex escape: \xHHH\ (optional terminator)
        if ch == "x":
            idx = next_idx + 1
            while (
                idx < len(prolog_code)
                and prolog_code[idx].lower() in "0123456789abcdef"
            ):
                idx += 1
            if idx < len(prolog_code) and prolog_code[idx] == "\\":
                idx += 1
            current.extend(prolog_code[start:idx])
            return idx

        # Unicode escape: \uXXXX
        if ch in {"u", "U"}:
            idx = min(len(prolog_code), next_idx + 5)
            current.extend(prolog_code[start:idx])
            return idx

        # Octal escape: \[0-7]{1,3}
        if ch in "01234567":
            idx = next_idx + 1
            while idx < len(prolog_code) and prolog_code[idx] in "01234567":
                idx += 1
            current.extend(prolog_code[start:idx])
            return idx

        # Single-character escape (including escaped quotes/backslashes)
        idx = next_idx + 1
        current.extend(prolog_code[start:idx])
        return idx

    def _is_edinburgh_radix_quote(index: int) -> bool:
        """Return True if apostrophe at ``index`` starts an Edinburgh radix literal.

        Detects forms like ``16'FF`` or ``-2'1010`` where the apostrophe should
        not open a quoted atom. We look backwards from the apostrophe to find
        a contiguous run of digits/underscores (optionally preceded by a sign)
        and ensure the next character begins the digit sequence.
        """

        # Need a following digit/letter/underscore to be a valid digits section
        if index + 1 >= len(prolog_code) or not (
            prolog_code[index + 1].isalnum() or prolog_code[index + 1] == "_"
        ):
            return False

        # Walk backwards to collect the radix digits
        j = index - 1
        while j >= 0 and (prolog_code[j].isdigit() or prolog_code[j] == "_"):
            j -= 1

        # Require at least one digit immediately before the apostrophe
        if j == index - 1:
            return False

        # Allow an optional sign directly before the digits
        if j >= 0 and prolog_code[j] in "+-":
            j -= 1

        # Disallow letters directly before the radix to avoid false positives
        if j >= 0 and (prolog_code[j].isalnum() or prolog_code[j] == "_"):
            return False

        return True

    while i < len(prolog_code):
        char = prolog_code[i]

        # Handle line comments
        if in_line_comment:
            current.append(char)
            if char == '\n':
                in_line_comment = False
            i += 1
            continue

        # Handle block comments
        if in_block_comment > 0:
            current.append(char)
            if prolog_code[i:i+2] == '/*':
                in_block_comment += 1
                current.append(prolog_code[i+1])
                i += 2
                continue
            if prolog_code[i:i+2] == '*/':
                in_block_comment -= 1
                current.append(prolog_code[i+1])
                i += 2
                continue
            i += 1
            continue

        # Handle escape sequences inside quoted text
        if char == '\\' and (in_single_quote or in_double_quote or in_char_code):
            i = _consume_escape_sequence(i)
            continue

        # Handle entering/exiting quotes
        if char == "'" and not in_double_quote:
            has_code = True
            if in_single_quote:
                # Check for doubled quote
                if i + 1 < len(prolog_code) and prolog_code[i + 1] == "'":
                    current.append(char)
                    current.append(prolog_code[i + 1])
                    i += 2
                    continue
                # Closing quote
                in_single_quote = False
                current.append(char)
                i += 1
                continue
            elif in_char_code:
                # Closing char code
                in_char_code = False
                current.append(char)
                i += 1
                continue
            elif (i > 0 and prolog_code[i - 1] == "0") or _is_edinburgh_radix_quote(i):
                # Starting char code
                in_char_code = True
                current.append(char)
                i += 1
                continue
            else:
                # Starting quoted atom
                in_single_quote = True
                current.append(char)
                i += 1
                continue

        if char == '"' and not in_single_quote:
            has_code = True
            in_double_quote = not in_double_quote
            current.append(char)
            i += 1
            continue

        # Check for comment starts
        if not in_single_quote and not in_double_quote:
            if char == '%':
                if not has_code and not current:
                    # Skip leading standalone comments so they don't attach to the
                    # next clause/directive chunk.
                    while i < len(prolog_code) and prolog_code[i] != '\n':
                        i += 1
                    continue
                in_line_comment = True
                current.append(char)
                i += 1
                continue
            # Check for block comment start: /* only starts a comment if NOT preceded
            # by '/' and not part of the literal '/*/'. E.g., '//*' and '/*/' remain operators.
            if prolog_code[i:i+2] == '/*':
                prev_char = prolog_code[i-1] if i > 0 else None
                if _should_start_block_comment(prolog_code, i, prev_char):
                    if not has_code and not current:
                        # Skip standalone block comments before any code in this chunk.
                        depth = 1
                        i += 2
                        while i < len(prolog_code) and depth > 0:
                            if prolog_code[i:i+2] == '/*':
                                depth += 1
                                i += 2
                                continue
                            if prolog_code[i:i+2] == '*/':
                                depth -= 1
                                i += 2
                                continue
                            i += 1
                        if depth > 0:
                            raise ValueError("Unterminated block comment")
                        continue
                    in_block_comment = 1
                    current.append(char)
                    current.append(prolog_code[i+1])
                    i += 2
                    continue

        # Track nesting depth for parentheses, brackets, braces
        if not in_single_quote and not in_double_quote:
            if char == '(':
                paren_depth += 1
            elif char == ')' and paren_depth > 0:
                paren_depth -= 1
            elif char == '[':
                bracket_depth += 1
            elif char == ']' and bracket_depth > 0:
                bracket_depth -= 1
            elif char == '{':
                brace_depth += 1
            elif char == '}' and brace_depth > 0:
                brace_depth -= 1

        # Handle period (end of clause)
        if char == '.' and not in_single_quote and not in_double_quote and paren_depth == 0 and bracket_depth == 0 and brace_depth == 0:
            # Check for ... (ellipsis) or .. (range operator)
            if prolog_code[i:i+3] == '...':
                current.append('.')
                current.append('.')
                current.append('.')
                has_code = True
                i += 3
                continue
            if prolog_code[i:i+2] == '..':
                current.append('.')
                current.append('.')
                has_code = True
                i += 2
                continue
            current.append(char)
            # Heuristic to check if this period is part of a number (e.g., 1.2 or 1.)
            # to avoid splitting clauses incorrectly.
            # The key insight: if the NEXT character is a digit, this period is part of
            # a decimal number (e.g., 3.14, or 2.3 in 1..2.3)
            # If the next character is NOT a digit (but previous is), it's a clause terminator
            current_str = ''.join(current)
            
            # Check if this could be a decimal point
            # ONLY if the next character is a digit
            is_decimal_point = (
                i + 1 < len(prolog_code) and prolog_code[i+1].isdigit()
            )
            if is_decimal_point:
                i += 1
                continue
            # End of clause
            if has_code:
                chunks.append(''.join(current))
            current = []
            has_code = False
            i += 1
            continue

        if not char.isspace():
            has_code = True

        current.append(char)
        i += 1

    # Check for unclosed comments or strings at end of code
    if in_single_quote:
        raise ValueError("Unterminated quoted atom")
    if in_double_quote:
        raise ValueError("Unterminated string")
    if in_block_comment > 0:
        raise ValueError("Unterminated block comment")
    
    # Add any remaining content (should not happen in well-formed code)
    if current:
        remainder = ''.join(current).strip()
        if remainder and has_code:
            chunks.append(remainder)
    
    return chunks


def _strip_quotes(token: str) -> str:
    if token.startswith("'") and token.endswith("'"):
        inner = token[1:-1]
        return inner.replace("''", "'")
    elif token.startswith('"') and token.endswith('"'):
        inner = token[1:-1]
        return inner.replace('\\"', '"').replace('\\\\', '\\')
    return token


def _split_top_level_commas(text: str) -> list[str]:
    parts: list[str] = []
    current: list[str] = []
    depth = 0
    in_single = False
    in_double = False
    escape_next = False

    for ch in text:
        if escape_next:
            current.append(ch)
            escape_next = False
            continue

        if ch == "\\" and (in_single or in_double):
            current.append(ch)
            escape_next = True
            continue

        if ch == "'" and not in_double:
            in_single = not in_single
            current.append(ch)
            continue
        if ch == '"' and not in_single:
            in_double = not in_double
            current.append(ch)
            continue

        if ch in "([{" and not in_single and not in_double:
            depth += 1
        elif ch in ")]}" and not in_single and not in_double and depth > 0:
            depth -= 1

        if ch == "," and not in_single and not in_double and depth == 0:
            parts.append("".join(current).strip())
            current = []
            continue

        current.append(ch)

    if current:
        parts.append("".join(current).strip())
    return parts


def _parse_operator_name_list(raw: str) -> list[str]:
    if raw.startswith("[") and raw.endswith("]"):
        inner = raw[1:-1]
        entries = _split_top_level_commas(inner)
    else:
        entries = [raw]

    operators: list[str] = []
    for entry in entries:
        if not entry:
            continue
        operators.append(_strip_quotes(entry))
    return operators


def _strip_comments(text: str) -> str:
    """Strip line and block comments from text, preserving content inside quoted strings.
    
    Args:
        text: Prolog source code that may contain comments
        
    Returns:
        Text with comments removed
    """
    result = []
    i = 0
    in_single_quote = False
    in_double_quote = False
    escape_next = False
    block_depth = 0
    
    while i < len(text):
        char = text[i]
        
        if escape_next:
            if block_depth == 0:
                result.append(char)
            escape_next = False
            i += 1
            continue
        
        if char == '\\' and (in_single_quote or in_double_quote):
            if block_depth == 0:
                result.append(char)
            escape_next = True
            i += 1
            continue
        
        # Handle block comments
        if block_depth > 0:
            if text[i:i+2] == '/*':
                block_depth += 1
                i += 2
                continue
            if text[i:i+2] == '*/':
                block_depth -= 1
                i += 2
                continue
            i += 1
            continue
        
        if char == "'" and not in_double_quote:
            # Check for doubled quote escape inside single-quoted strings
            if in_single_quote and i + 1 < len(text) and text[i + 1] == "'":
                result.append(char)
                result.append(text[i + 1])
                i += 2
                continue
            in_single_quote = not in_single_quote
            result.append(char)
            i += 1
            continue
        
        if char == '"' and not in_single_quote:
            in_double_quote = not in_double_quote
            result.append(char)
            i += 1
            continue
        
        # Check for comment starts outside quoted strings
        if not in_single_quote and not in_double_quote:
            # Check for block comment start: /* only starts a comment if NOT preceded
            # by '/' and not part of the literal '/*/'.
            if text[i:i+2] == '/*':
                prev_char = result[-1] if result else None
                if _should_start_block_comment(text, i, prev_char):
                    block_depth = 1
                    i += 2
                    # Insert a space to prevent token gluing after the comment
                    # (e.g., /* comment */true should become ' true' not 'true' glued to prior token)
                    result.append(' ')
                    continue
            # Check for line comment
            if char == '%':
                # Skip until end of line
                while i < len(text) and text[i] != '\n':
                    i += 1
                # Skip the newline if present
                if i < len(text) and text[i] == '\n':
                    result.append('\n')
                    i += 1
                continue
        
        result.append(char)
        i += 1
    
    return ''.join(result)


def extract_op_directives(source: str) -> list[tuple[int, str, str]]:
    """Extract op/3 directives from source code, including operators in module export lists.

    Args:
        source: Prolog source code string

    Returns:
        List of (precedence, type, operator) tuples, in order of appearance
    """

    operators: list[tuple[int, str, str]] = []
    directive_pattern = re.compile(r"^\s*:-\s*op\s*\((.*)\)\s*\.$", re.DOTALL)
    module_pattern = re.compile(r"^\s*:-\s*module\s*\(\s*([^,]+)\s*,\s*\[(.*)\]\s*\)\s*\.$", re.DOTALL)

    def _try_parse_op(op_args_str: str):
        """Helper to parse op/3 arguments and add to operators list."""
        args = _split_top_level_commas(op_args_str)
        if len(args) != 3:
            return
        try:
            precedence = int(args[0].strip())
        except ValueError:
            return
        op_type = _strip_quotes(args[1].strip())
        operator_names = _parse_operator_name_list(args[2].strip())
        for op_name in operator_names:
            operators.append((precedence, op_type, op_name))

    # Strip comments once before tokenizing so the tokenizer never sees unterminated
    # quoted atoms that only occur inside comments (e.g., documentation blocks).
    cleaned_source = _strip_comments(source)

    for statement in tokenize_prolog_statements(cleaned_source):
        stripped = statement.strip()
        
        # Check for op/3 directives
        match = directive_pattern.match(stripped)
        if match:
            _try_parse_op(match.group(1))
            continue

        # Check for module/2 with op/3 in export list
        match = module_pattern.match(stripped)
        if match:
            exports_str = match.group(2)
            export_items = _split_top_level_commas(exports_str)
            for item in export_items:
                item = item.strip()
                if item.startswith("op(") and item.endswith(")"):
                    op_content = item[3:-1]  # Remove "op(" and ")"
                    _try_parse_op(op_content)

    return operators


def escape_for_lark(s: str) -> str:
    """Escape special characters for use in Lark grammar.
    
    Args:
        s: String to escape
        
    Returns:
        Escaped string safe for Lark grammar
    """
    # Escape backslashes and quotes
    s = s.replace("\\", "\\\\")
    s = s.replace('"', '\\"')
    return s


VALID_OPERATOR_SPECS = {"xfx", "xfy", "yfx", "yfy", "fx", "fy", "xf", "yf"}


def _format_operator_literals(ops: Iterable[str]) -> str:
    # Sort longest operators first so sequences like "\\+" take precedence over
    # shorter prefixes such as "\\".
    formatted: list[str] = []
    for op in sorted(set(ops), key=lambda value: (-len(value), value)):
        if re.match(r"^[A-Za-z0-9_]+$", op):
            formatted.append(f"/(?<![A-Za-z0-9_]){re.escape(op)}(?![A-Za-z0-9_])/")
        else:
            formatted.append(f'"{escape_for_lark(op)}"')
    return " | ".join(formatted)


def _operator_token_priority(name: str) -> int:
    """Choose a token priority that beats the generic OP_SYMBOL lexer rule.

    OP_SYMBOL has priority 25 and eagerly matches runs of operator punctuation,
    which would swallow custom operators like +++ or ***, preventing the
    operator-specific tokens from ever being seen by the parser. By giving
    punctuation-heavy operators a priority above 25 we ensure the generated
    operator tokens win when they are valid in the current parse context.
    """
    if re.match(r"^[A-Za-z0-9_]+$", name):
        return max(len(name), 1)
    if len(name) > 1:
        return 30 + len(name)
    return 1


def _merge_operators(
    base_ops: Iterable[tuple[int, str, str]], directives: list[tuple[int, str, str]]
) -> list[tuple[int, str, str]]:
    table: dict[tuple[str, str], int] = {}
    for precedence, spec, name in base_ops:
        if spec not in VALID_OPERATOR_SPECS:
            continue
        table[(name, spec)] = precedence

    for precedence, spec, name in directives:
        if spec not in VALID_OPERATOR_SPECS:
            continue
        key = (name, spec)
        if precedence == 0:
            table.pop(key, None)
        else:
            table[key] = precedence

    merged: list[tuple[int, str, str]] = []
    for (name, spec), precedence in table.items():
        merged.append((precedence, spec, name))
    merged.sort(key=lambda item: (item[0], item[1], item[2]))
    return merged


def generate_operator_rules(operators: list[tuple[int, str, str]]) -> str:
    """Generate grammar rules for operators ordered by precedence.

    ISO Prolog uses 1 as highest precedence. The generated rules build from
    highest binding (small number) to lowest (large number) so the resulting
    ``term`` rule respects the intended precedence tree.
    """

    if not operators:
        return "    ?term: primary\n"

    grouped: dict[int, dict[str, list[str]]] = defaultdict(
        lambda: {"prefix": defaultdict(list), "infix": defaultdict(list), "postfix": defaultdict(list)}
    )

    for precedence, spec, name in operators:
        if spec in {"fx", "fy"}:
            grouped[precedence]["prefix"][spec].append(name)
        elif spec in {"xf", "yf"}:
            grouped[precedence]["postfix"][spec].append(name)
        else:
            grouped[precedence]["infix"][spec].append(name)

    rules: list[str] = []
    tokens: list[str] = []

    lower_rule = "primary"
    precedence_levels = sorted(grouped.keys())

    token_counter = 0

    for precedence in precedence_levels:
        rule_name = f"level_{precedence}"
        parts = [lower_rule]
        infix_specs = grouped[precedence]["infix"]
        prefix_specs = grouped[precedence]["prefix"]
        postfix_specs = grouped[precedence]["postfix"]

        if infix_specs.get("xfx"):
            for name in sorted(infix_specs["xfx"]):
                token_counter += 1
                token = f"INFIX_XFX_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{lower_rule} {token} {lower_rule} -> infix_xfx")
        if infix_specs.get("yfx"):
            for name in sorted(infix_specs["yfx"]):
                token_counter += 1
                token = f"INFIX_YFX_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{rule_name} {token} {lower_rule} -> infix_yfx")
        if infix_specs.get("xfy"):
            for name in sorted(infix_specs["xfy"]):
                token_counter += 1
                token = f"INFIX_XFY_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{lower_rule} {token} {rule_name} -> infix_xfy")
        if infix_specs.get("yfy"):
            for name in sorted(infix_specs["yfy"]):
                token_counter += 1
                token = f"INFIX_YFY_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{rule_name} {token} {rule_name} -> infix_yfy")

        if prefix_specs.get("fx"):
            for name in sorted(prefix_specs["fx"]):
                token_counter += 1
                token = f"PREFIX_FX_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{token} {lower_rule} -> prefix_fx")
        if prefix_specs.get("fy"):
            for name in sorted(prefix_specs["fy"]):
                token_counter += 1
                token = f"PREFIX_FY_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{token} {rule_name} -> prefix_fy")

        if postfix_specs.get("xf"):
            for name in sorted(postfix_specs["xf"]):
                token_counter += 1
                token = f"POSTFIX_XF_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{lower_rule} {token} -> postfix_xf")
        if postfix_specs.get("yf"):
            for name in sorted(postfix_specs["yf"]):
                token_counter += 1
                token = f"POSTFIX_YF_{precedence}_{token_counter}"
                priority = _operator_token_priority(name)
                token_def = f"{token}.{priority}" if priority else token
                tokens.append(f"    {token_def}: {_format_operator_literals([name])}")
                parts.append(f"{rule_name} {token} -> postfix_yf")

        rule_body = "\n        | ".join(parts)
        rules.append(f"?{rule_name}: {rule_body}")
        lower_rule = rule_name

    rules.append(f"?term: {lower_rule}")
    rules.extend(tokens)

    return "\n".join(rules) + "\n"


class PrologParser:
    """Parse Prolog source code with support for dynamic operators."""

    def __init__(self, operator_table=None):
        self.operator_table = operator_table
        # Character conversion table: maps single chars to single chars
        # Initially identity (no conversions active)
        self._char_conversions: dict[str, str] = {}
        self._grammar_cache = {}  # Cache compiled grammars by operator set
        self.parser = None

    def set_char_conversion(self, from_char: str, to_char: str) -> None:
        """Set a character conversion.
        
        If from_char == to_char, removes the conversion (identity mapping).
        Otherwise, adds/updates the conversion.
        """
        if from_char == to_char:
            # Identity conversion: remove from table
            self._char_conversions.pop(from_char, None)
        else:
            self._char_conversions[from_char] = to_char

    def get_char_conversions(self) -> dict[str, str]:
        """Return copy of current character conversion table."""
        return dict(self._char_conversions)

    def convert_atom_name(self, name: str) -> str:
        """Apply character conversions to an atom/functor name."""
        if not self._char_conversions:
            return name
        return ''.join(self._char_conversions.get(ch, ch) for ch in name)

    def _apply_char_conversions(self, text: str) -> str:
        """Apply character conversions to text, excluding quoted strings.
        
        Per ISO Prolog, character conversions apply to source text but NOT
        to the contents of quoted atoms or strings.
        """
        if not self._char_conversions:
            return text
        
        result = []
        i = 0
        in_single_quote = False
        in_double_quote = False
        escape_next = False
        
        while i < len(text):
            char = text[i]
            
            if escape_next:
                result.append(char)
                escape_next = False
                i += 1
                continue
            
            if char == '\\' and (in_single_quote or in_double_quote):
                result.append(char)
                escape_next = True
                i += 1
                continue
            
            if char == "'" and not in_double_quote:
                # Check for doubled quote escape inside single-quoted strings
                if in_single_quote and i + 1 < len(text) and text[i + 1] == "'":
                    result.append(char)
                    result.append(text[i + 1])
                    i += 2
                    continue
                in_single_quote = not in_single_quote
                result.append(char)
                i += 1
                continue
            
            if char == '"' and not in_single_quote:
                in_double_quote = not in_double_quote
                result.append(char)
                i += 1
                continue
            
            # Apply conversion only outside quoted strings
            if not in_single_quote and not in_double_quote:
                char = self._char_conversions.get(char, char)
            
            result.append(char)
            i += 1
        
        return ''.join(result)

    def _create_parser(self, grammar: str):
        return Lark(
            grammar,
            parser="earley",
            propagate_positions=True,
            ambiguity='resolve',
            start=["start", "clause", "directive"],
        )

    def _base_operator_definitions(
        self, module_name: str | None = None
    ) -> list[tuple[int, str, str]]:
        if self.operator_table is None:
            return list(DEFAULT_OPERATORS)
        # Use module-aware operator iteration to include shadowed operators
        return [
            (info.precedence, info.spec, name)
            for name, info in self.operator_table.iter_operators_for_module(module_name)
        ]

    def _build_grammar(self, operators: list[tuple[int, str, str]]) -> str:
        operator_rules = generate_operator_rules(operators)
        return PROLOG_GRAMMAR.replace("__OPERATOR_GRAMMAR__", operator_rules)

    def _ensure_parser(
        self,
        cleaned_text: str,
        directive_ops: list[tuple[int, str, str]] | None = None,
        module_name: str | None = None,
    ) -> None:
        if directive_ops is None:
            try:
                directive_ops = extract_op_directives(cleaned_text)
            except ValueError:
                directive_ops = []
        operators = _merge_operators(
            self._base_operator_definitions(module_name), directive_ops
        )
        key = tuple(operators)
        if key not in self._grammar_cache:
            grammar = self._build_grammar(operators)
            self._grammar_cache[key] = self._create_parser(grammar)
        self.parser = self._grammar_cache[key]

    def _strip_block_comments(self, text: str) -> tuple[str, list[tuple[int, str]]]:
        """Strip block comments from text, handling nesting and quoted strings.
        Returns cleaned text and list of (position, pldoc_comment) tuples."""
        result = []
        pldoc_comments = []
        i = 0
        in_single_quote = False
        in_double_quote = False
        escape_next = False
        while i < len(text):
            if not in_single_quote and not in_double_quote and text.startswith('/*', i):
                prev_char = result[-1] if result else None
                if not _should_start_block_comment(text, i, prev_char):
                    # Not a comment, just part of an operator
                    result.append(text[i])
                    i += 1
                    continue
                
                comment_start = i
                depth = 1
                # PlDoc comments start with /** or /*! but NOT /**/ (which is empty block)
                is_pldoc = (
                    (text.startswith('/**', i) and not text.startswith('/**/', i))
                    or (text.startswith('/*!', i) and not text.startswith('/*!/', i))
                )
                if is_pldoc:
                    i += 3  # Skip /** or /*!
                else:
                    i += 2  # Skip /*
                comment_content = []
                while i < len(text) and depth > 0:
                    if text.startswith('/*', i):
                        depth += 1
                        i += 2
                    elif text.startswith('*/', i):
                        depth -= 1
                        if depth == 0:
                            # End of comment
                            if is_pldoc:
                                pldoc_comments.append((comment_start, ''.join(comment_content)))
                            i += 2
                            # Insert a space to prevent token gluing
                            result.append(' ')
                            break
                        else:
                            i += 2
                    else:
                        if is_pldoc:
                            comment_content.append(text[i])
                        i += 1
                if depth > 0:
                    raise ValueError("Unterminated block comment")
                continue  # Skip the comment

            char = text[i]
            result.append(char)

            if escape_next:
                escape_next = False
            elif char == '\\':
                escape_next = True
            elif char == "'" and not in_double_quote:
                in_single_quote = not in_single_quote
            elif char == '"' and not in_single_quote:
                in_double_quote = not in_double_quote

            i += 1
        return ''.join(result), pldoc_comments

    def _collect_pldoc_comments(self, text: str) -> tuple[str, list[tuple[int, str]]]:
        """Collect PlDoc comments and return cleaned text with positions in cleaned text."""
        pldoc_comments = []
        cleaned = []
        i = 0
        in_single_quote = False
        in_double_quote = False
        escape_next = False
        while i < len(text):
            if not in_single_quote and not in_double_quote:
                if text.startswith('%%', i):
                    # Line comment
                    pos = len(''.join(cleaned))  # position in cleaned text
                    # Find end of line
                    end = text.find('\n', i)
                    if end == -1:
                        end = len(text)
                    comment = text[i+2:end].rstrip('\n')
                    pldoc_comments.append((pos, comment))
                    i = end + 1 if end < len(text) else len(text)
                    continue
                elif text.startswith('/*', i):
                    # Block comment: /* only starts a comment if NOT preceded by '/'
                    # and not part of the literal '/*/'.
                    prev_char = cleaned[-1] if cleaned else None
                    if _should_start_block_comment(text, i, prev_char):
                        # Block comment
                        pos = len(''.join(cleaned))
                        # PlDoc comments start with /** or /*! but NOT /**/ (which is empty block)
                        is_pldoc = (
                            (text.startswith('/**', i) and not text.startswith('/**/', i))
                            or (text.startswith('/*!', i) and not text.startswith('/*!/', i))
                        )
                        i += 3 if is_pldoc else 2
                        comment_content = []
                        depth = 1
                        while i < len(text) and depth > 0:
                            if text.startswith('/*', i):
                                depth += 1
                                i += 2
                            elif text.startswith('*/', i):
                                depth -= 1
                                if depth == 0:
                                    if is_pldoc:
                                        pldoc_comments.append((pos, ''.join(comment_content)))
                                    i += 2
                                    # Insert a space to prevent token gluing
                                    cleaned.append(' ')
                                    break
                                else:
                                    i += 2
                            else:
                                if is_pldoc:
                                    comment_content.append(text[i])
                                i += 1
                        if depth > 0:
                            raise ValueError("Unterminated block comment")
                        continue
            # Not in comment, add char
            cleaned.append(text[i])
            if escape_next:
                escape_next = False
            elif text[i] == '\\':
                escape_next = True
            elif text[i] == "'" and not in_double_quote:
                in_single_quote = not in_single_quote
            elif text[i] == '"' and not in_single_quote:
                in_double_quote = not in_double_quote
            i += 1
        cleaned_text = ''.join(cleaned)
        pldoc_comments.sort(key=lambda x: x[0])
        return cleaned_text, pldoc_comments

    def _associate_pldoc_comments(self, items: list, comments: list[tuple[int, str]]):
        """Associate PlDoc comments with clauses/directives."""
        if not comments:
            return

        # Robust association: attach each comment to the next item by source position.
        # We rely on Lark's propagate_positions to furnish item.meta.start_pos where available.
        entities: list[tuple[int, str, object]] = []
        for pos, text in comments:
            entities.append((pos, 'comment', text))
        for item in items:
            start_pos = getattr(item.meta, 'start_pos', None) if hasattr(item, 'meta') else None
            if start_pos is not None:
                entities.append((start_pos, 'item', item))

        entities.sort(key=lambda x: x[0])
        last_comment_text: str | None = None
        for _, entity_type, payload in entities:
            if entity_type == 'comment':
                last_comment_text = payload
            elif entity_type == 'item':
                if last_comment_text is not None:
                    payload.doc = last_comment_text
                last_comment_text = None

    def parse(
        self,
        text: str,
        context: str = "parse/1",
        *,
        apply_char_conversions: bool = True,
        directive_ops: list[tuple[int, str, str]] | None = None,
        module_name: str | None = None,
    ) -> list[Clause | Directive]:
        """Parse Prolog source code and return list of clauses.

        Args:
            text: The Prolog source code to parse
            context: Context string for error messages
            apply_char_conversions: Whether to apply character conversions
            directive_ops: Pre-extracted operator directives
            module_name: Current module context for module-scoped operators
        """
        try:
            # Apply character conversions before parsing
            if apply_char_conversions:
                text = self._apply_char_conversions(text)
            cleaned_text, pldoc_comments = self._collect_pldoc_comments(text)
            self._ensure_parser(cleaned_text, directive_ops, module_name)
            transformer = PrologTransformer()
            parsed_items: list[Clause | Directive] = []

            # Parse each clause/directive separately to avoid Earley state explosion
            # on large files with many clauses.
            statements = tokenize_prolog_statements(cleaned_text)
            search_pos = 0
            for statement in statements:
                # Track position of the statement within the cleaned text so that
                # PlDoc association via start_pos still works.
                stmt_pos = cleaned_text.find(statement, search_pos)
                if stmt_pos == -1:
                    raise RuntimeError(
                        f"Could not re-locate tokenized statement in text: {statement!r}"
                    )
                search_pos = stmt_pos + len(statement)

                stripped = statement.lstrip()
                start_rule = "directive" if stripped.startswith(":-") else "clause"
                try:
                    tree = self.parser.parse(statement, start=start_rule)
                except LarkError:
                    tree = self.parser.parse(statement, start="start")
                transformed = transformer.transform(tree)
                if isinstance(transformed, list):
                    parsed = transformed
                else:
                    parsed = [transformed]

                items = [self._fold_numeric_unary_minus(item) for item in parsed]
                for item in items:
                    meta = getattr(item, "meta", None)
                    if meta is not None and hasattr(meta, "start_pos"):
                        meta.start_pos += stmt_pos
                parsed_items.extend(items)
            # Associate PlDoc comments with items
            self._associate_pldoc_comments(parsed_items, pldoc_comments)
            return parsed_items
        except (UnexpectedToken, UnexpectedCharacters) as e:
            # If the lexer/parser choked inside a char code hex escape like 0'\x4G,
            # surface the ISO-style unexpected_char error rather than the raw Lark token message.
            last_token = getattr(e, "token_history", None)
            if last_token:
                last_token = last_token[-1]
                if getattr(last_token, "type", None) == "CHAR_CODE" and str(last_token).startswith(
                    "0'\\x"
                ):
                    error_term = PrologError.syntax_error("unexpected_char", context)
                    raise PrologThrow(error_term)
            error_term = PrologError.syntax_error(str(e), context)
            # We handle these specific Lark errors here so they are normalized to
            # `PrologThrow` before control leaves the try; otherwise the outer
            # `LarkError` handler never runs and tests break.
            raise PrologThrow(error_term)
        except LarkError as e:
            # Convert Lark parse error to Prolog syntax_error
            error_term = PrologError.syntax_error(str(e), context)
            raise PrologThrow(error_term)
        except ValueError as e:
            # Unterminated comment
            error_term = PrologError.syntax_error(str(e), context)
            raise PrologThrow(error_term)

    def _fold_numeric_unary_minus(self, term):
        """Normalize unary minus applications to numeric literals."""

        if isinstance(term, Clause):
            head = self._fold_numeric_unary_minus(term.head)
            body = None
            if term.body is not None:
                body = [self._fold_numeric_unary_minus(goal) for goal in term.body]
            return Clause(head, body, term.doc, term.meta, term.dcg)

        if isinstance(term, Directive):
            goal = self._fold_numeric_unary_minus(term.goal)
            return Directive(goal, term.doc, term.meta)

        if isinstance(term, List):
            elements = tuple(self._fold_numeric_unary_minus(e) for e in term.elements)
            tail = None if term.tail is None else self._fold_numeric_unary_minus(term.tail)
            return List(elements, tail)

        if isinstance(term, Compound):
            args = tuple(self._fold_numeric_unary_minus(arg) for arg in term.args)
            if term.functor == "-" and len(args) == 1 and isinstance(args[0], Number):
                return Number(-args[0].value)
            return Compound(term.functor, args)

        return term

    def parse_term(
        self,
        text: str,
        context: str = "parse_term/1",
        *,
        apply_char_conversions: bool = True,
    ) -> Any:
        """Parse a single Prolog term."""
        try:
            # Add a period to make it a valid clause
            clause_text = f"dummy({text})."
            if apply_char_conversions:
                clause_text = self._apply_char_conversions(clause_text)
            cleaned_text, _ = self._collect_pldoc_comments(clause_text)
            self._ensure_parser(cleaned_text)
            tree = self.parser.parse(cleaned_text, start="start")
            transformer = PrologTransformer()
            result = transformer.transform(tree)
            if result and isinstance(result[0], Clause):
                compound = result[0].head
                if isinstance(compound, Compound) and compound.args:
                    return compound.args[0]
            raise ValueError(f"Failed to parse term: {text}")
        except LarkError as e:
            # Convert Lark parse error to Prolog syntax_error
            error_term = PrologError.syntax_error(str(e), context)
            raise PrologThrow(error_term)
        except ValueError as e:
            # Unterminated comment or parse error
            error_term = PrologError.syntax_error(str(e), context)
            raise PrologThrow(error_term)
